#!/usr/bin/env python3
"""
ç®€åŒ–çš„éŸ³é¢‘æ–‡æœ¬å¯¹é½å·¥å…·
ä¸“é—¨ç”¨äºå°†ç°æœ‰MP3éŸ³é¢‘ä¸TXTæ–‡æœ¬å¯¹é½ç”Ÿæˆå­—å¹•
"""

import os
import re
from faster_whisper import WhisperModel
from moviepy.editor import AudioFileClip
from loguru import logger

def split_text_by_punctuation(text):
    """æŒ‰æ ‡ç‚¹ç¬¦å·æ™ºèƒ½åˆ†å‰²æ–‡æœ¬"""
    # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²ï¼Œä½†ä¿æŒåˆç†çš„å¥å­é•¿åº¦
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›]', text)
    
    result = []
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # å¦‚æœå¥å­å¤ªé•¿ï¼ŒæŒ‰é€—å·è¿›ä¸€æ­¥åˆ†å‰²
        if len(sentence) > 50:
            parts = re.split(r'[ï¼Œ,]', sentence)
            current = ""
            for part in parts:
                part = part.strip()
                if not part:
                    continue
                    
                if len(current + part) < 50:
                    current += part + "ï¼Œ"
                else:
                    if current:
                        result.append(current.rstrip("ï¼Œ"))
                    current = part + "ï¼Œ"
            
            if current:
                result.append(current.rstrip("ï¼Œ"))
        else:
            result.append(sentence)
    
    return [s for s in result if s.strip()]

def transcribe_audio_with_timestamps(audio_file, model_size="large-v3"):
    """è½¬å½•éŸ³é¢‘å¹¶è·å–æ®µè½çº§æ—¶é—´æˆ³"""
    logger.info(f"å¼€å§‹è½¬å½•éŸ³é¢‘: {audio_file}")
    
    model = WhisperModel(model_size, device="cpu", compute_type="int8")
    
    segments, info = model.transcribe(
        audio_file,
        beam_size=5,
        word_timestamps=False,  # ä½¿ç”¨æ®µè½çº§æ—¶é—´æˆ³ï¼Œæ›´ç¨³å®š
        vad_filter=True,
        vad_parameters=dict(min_silence_duration_ms=500),
    )
    
    logger.info(f"æ£€æµ‹è¯­è¨€: {info.language}, ç½®ä¿¡åº¦: {info.language_probability:.2f}")
    
    transcribed_segments = []
    for segment in segments:
        transcribed_segments.append({
            'text': segment.text.strip(),
            'start': segment.start,
            'end': segment.end
        })
    
    return transcribed_segments, info.language

def calculate_text_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼ï¼Œè½¬ä¸ºå°å†™
    clean1 = re.sub(r'[^\w]', '', text1.lower())
    clean2 = re.sub(r'[^\w]', '', text2.lower())
    
    if not clean1 or not clean2:
        return 0.0
    
    # è®¡ç®—å­—ç¬¦çº§ç›¸ä¼¼åº¦
    shorter = min(len(clean1), len(clean2))
    longer = max(len(clean1), len(clean2))
    
    if longer == 0:
        return 1.0
    
    # è®¡ç®—å…¬å…±å­—ç¬¦æ•°
    common_chars = 0
    for i in range(shorter):
        if i < len(clean1) and i < len(clean2) and clean1[i] == clean2[i]:
            common_chars += 1
    
    return common_chars / longer

def align_text_with_audio(txt_segments, transcribed_segments):
    """å°†æ–‡æœ¬æ®µè½ä¸éŸ³é¢‘è½¬å½•å¯¹é½"""
    logger.info(f"å¼€å§‹å¯¹é½ {len(txt_segments)} ä¸ªæ–‡æœ¬æ®µè½å’Œ {len(transcribed_segments)} ä¸ªéŸ³é¢‘æ®µè½")
    
    aligned_segments = []
    
    # è·å–éŸ³é¢‘æ€»æ—¶é•¿
    if transcribed_segments:
        total_audio_duration = transcribed_segments[-1]['end']
    else:
        total_audio_duration = 60.0  # é»˜è®¤60ç§’
    
    # å¦‚æœæ–‡æœ¬æ®µè½æ•°é‡ä¸éŸ³é¢‘æ®µè½æ•°é‡ç›¸è¿‘ï¼Œç›´æ¥å¯¹åº”
    if abs(len(txt_segments) - len(transcribed_segments)) <= 2:
        logger.info("æ®µè½æ•°é‡ç›¸è¿‘ï¼Œä½¿ç”¨ç›´æ¥å¯¹åº”ç­–ç•¥")
        
        for i, txt_segment in enumerate(txt_segments):
            if i < len(transcribed_segments):
                # ä½¿ç”¨éŸ³é¢‘çš„æ—¶é—´æˆ³
                audio_seg = transcribed_segments[i]
                aligned_segments.append({
                    'text': txt_segment,
                    'start_time': audio_seg['start'],
                    'end_time': audio_seg['end'],
                    'confidence': calculate_text_similarity(txt_segment, audio_seg['text'])
                })
            else:
                # è¶…å‡ºéƒ¨åˆ†ä½¿ç”¨ä¼°ç®—æ—¶é—´
                if aligned_segments:
                    last_end = aligned_segments[-1]['end_time']
                    duration = len(txt_segment) * 0.15  # ä¼°ç®—æ¯å­—ç¬¦0.15ç§’
                    aligned_segments.append({
                        'text': txt_segment,
                        'start_time': last_end,
                        'end_time': min(last_end + duration, total_audio_duration),
                        'confidence': 0.5
                    })
    
    else:
        # æ®µè½æ•°é‡å·®å¼‚è¾ƒå¤§ï¼Œä½¿ç”¨æ—¶é—´æ¯”ä¾‹åˆ†é…
        logger.info("æ®µè½æ•°é‡å·®å¼‚è¾ƒå¤§ï¼Œä½¿ç”¨æ—¶é—´æ¯”ä¾‹åˆ†é…ç­–ç•¥")
        
        # è®¡ç®—æ¯ä¸ªæ–‡æœ¬æ®µè½çš„ç›¸å¯¹æƒé‡ï¼ˆåŸºäºå­—ç¬¦æ•°ï¼‰
        total_chars = sum(len(seg) for seg in txt_segments)
        
        current_time = 0.0
        for i, txt_segment in enumerate(txt_segments):
            char_ratio = len(txt_segment) / total_chars if total_chars > 0 else 1.0 / len(txt_segments)
            duration = total_audio_duration * char_ratio
            
            # ç¡®ä¿æœ€å°å’Œæœ€å¤§æ—¶é•¿
            duration = max(1.0, min(duration, 10.0))
            
            end_time = min(current_time + duration, total_audio_duration)
            
            aligned_segments.append({
                'text': txt_segment,
                'start_time': current_time,
                'end_time': end_time,
                'confidence': 0.7  # ä¼°ç®—ç½®ä¿¡åº¦
            })
            
            current_time = end_time
            
            if current_time >= total_audio_duration:
                break
    
    return aligned_segments

def generate_srt_file(aligned_segments, output_file):
    """ç”ŸæˆSRTå­—å¹•æ–‡ä»¶"""
    def format_timestamp(seconds):
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        milliseconds = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{milliseconds:03d}"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for i, segment in enumerate(aligned_segments, 1):
            start_time = format_timestamp(segment['start_time'])
            end_time = format_timestamp(segment['end_time'])
            
            f.write(f"{i}\n")
            f.write(f"{start_time} --> {end_time}\n")
            f.write(f"{segment['text']}\n\n")
    
    logger.info(f"SRTå­—å¹•æ–‡ä»¶å·²ç”Ÿæˆ: {output_file}")
    return output_file

def align_mp3_with_txt(mp3_file, txt_file, output_srt_file):
    """
    ä¸»å‡½æ•°ï¼šå°†MP3éŸ³é¢‘ä¸TXTæ–‡æœ¬å¯¹é½ç”ŸæˆSRTå­—å¹•
    
    Args:
        mp3_file: MP3éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        txt_file: TXTæ–‡æœ¬æ–‡ä»¶è·¯å¾„
        output_srt_file: è¾“å‡ºSRTå­—å¹•æ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: å¯¹é½ç»“æœæŠ¥å‘Š
    """
    
    logger.info("ğŸš€ å¼€å§‹éŸ³é¢‘æ–‡æœ¬å¯¹é½æµç¨‹")
    
    # 1. æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
    if not os.path.exists(mp3_file):
        raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {mp3_file}")
    
    if not os.path.exists(txt_file):
        raise FileNotFoundError(f"æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {txt_file}")
    
    # 2. è¯»å–å¹¶åˆ†å‰²æ–‡æœ¬
    logger.info("ğŸ“– è¯»å–æ–‡æœ¬æ–‡ä»¶...")
    with open(txt_file, 'r', encoding='utf-8') as f:
        txt_content = f.read().strip()
    
    if not txt_content:
        raise ValueError("æ–‡æœ¬æ–‡ä»¶ä¸ºç©º")
    
    txt_segments = split_text_by_punctuation(txt_content)
    logger.info(f"æ–‡æœ¬åˆ†å‰²ä¸º {len(txt_segments)} ä¸ªæ®µè½")
    
    # 3. è½¬å½•éŸ³é¢‘
    logger.info("ğŸµ è½¬å½•éŸ³é¢‘æ–‡ä»¶...")
    transcribed_segments, language = transcribe_audio_with_timestamps(mp3_file)
    logger.info(f"éŸ³é¢‘è½¬å½•ä¸º {len(transcribed_segments)} ä¸ªæ®µè½")
    
    # 4. å¯¹é½æ–‡æœ¬å’ŒéŸ³é¢‘
    logger.info("ğŸ”„ å¯¹é½æ–‡æœ¬å’ŒéŸ³é¢‘...")
    aligned_segments = align_text_with_audio(txt_segments, transcribed_segments)
    
    # 5. ç”ŸæˆSRTæ–‡ä»¶
    logger.info("ğŸ“ ç”ŸæˆSRTå­—å¹•æ–‡ä»¶...")
    srt_file = generate_srt_file(aligned_segments, output_srt_file)
    
    # 6. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    total_confidence = sum(seg['confidence'] for seg in aligned_segments)
    avg_confidence = total_confidence / len(aligned_segments) if aligned_segments else 0
    
    total_duration = aligned_segments[-1]['end_time'] if aligned_segments else 0
    
    report = {
        'success': True,
        'mp3_file': mp3_file,
        'txt_file': txt_file,
        'srt_file': srt_file,
        'language': language,
        'total_segments': len(aligned_segments),
        'total_duration': total_duration,
        'average_confidence': avg_confidence,
        'segments_preview': aligned_segments[:3]  # å‰3ä¸ªæ®µè½é¢„è§ˆ
    }
    
    logger.info(f"âœ… å¯¹é½å®Œæˆ! ç”Ÿæˆ {len(aligned_segments)} ä¸ªå­—å¹•æ®µè½")
    logger.info(f"ğŸ“Š å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")
    logger.info(f"â±ï¸ æ€»æ—¶é•¿: {total_duration:.2f} ç§’")
    
    return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    mp3_file = "your_audio.mp3"      # æ›¿æ¢ä¸ºä½ çš„MP3æ–‡ä»¶è·¯å¾„
    txt_file = "your_script.txt"     # æ›¿æ¢ä¸ºä½ çš„TXTæ–‡ä»¶è·¯å¾„
    output_srt = "aligned_subtitle.srt"  # è¾“å‡ºçš„SRTæ–‡ä»¶è·¯å¾„
    
    try:
        result = align_mp3_with_txt(mp3_file, txt_file, output_srt)
        
        print("ğŸ‰ éŸ³é¢‘æ–‡æœ¬å¯¹é½æˆåŠŸ!")
        print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {result['srt_file']}")
        print(f"ğŸŒ æ£€æµ‹è¯­è¨€: {result['language']}")
        print(f"ğŸ“Š å­—å¹•æ®µè½: {result['total_segments']} ä¸ª")
        print(f"â±ï¸ æ€»æ—¶é•¿: {result['total_duration']:.1f} ç§’")
        print(f"ğŸ¯ å¹³å‡ç½®ä¿¡åº¦: {result['average_confidence']:.2f}")
        
        print("\nğŸ“‹ å‰3ä¸ªæ®µè½é¢„è§ˆ:")
        for i, segment in enumerate(result['segments_preview']):
            print(f"  {i+1}. [{segment['start_time']:.1f}s - {segment['end_time']:.1f}s] "
                  f"ç½®ä¿¡åº¦: {segment['confidence']:.2f}")
            print(f"     {segment['text'][:60]}...")
        
    except Exception as e:
        print(f"âŒ å¯¹é½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()