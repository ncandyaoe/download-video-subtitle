#!/usr/bin/env python3
"""
è§†é¢‘å¤„ç†APIæ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿæ€§èƒ½ã€ååé‡ã€å“åº”æ—¶é—´å’Œèµ„æºä½¿ç”¨æ•ˆç‡
"""

import unittest
import requests
import time
import threading
import statistics
import psutil
import json
import concurrent.futures
from typing import List, Dict, Any, Tuple
from datetime import datetime, timedelta
import sys

class PerformanceBenchmarkTester:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å™¨"""
    
    def __init__(self, api_base_url: str = "http://localhost:7878"):
        self.api_base_url = api_base_url
        self.session = requests.Session()
        self.test_tasks = []
        self.performance_data = {
            'response_times': [],
            'throughput': [],
            'resource_usage': [],
            'error_rates': []
        }
        
        # æµ‹è¯•é…ç½®
        self.test_video_url = "https://www.youtube.com/watch?v=dQw4w9WgXcQ"
        
    def cleanup(self):
        """æ¸…ç†æµ‹è¯•èµ„æº"""
        for task_id in self.test_tasks:
            try:
                self.session.post(f"{self.api_base_url}/system/tasks/{task_id}/cancel")
            except:
                pass
    
    def check_api_availability(self) -> bool:
        """æ£€æŸ¥APIæ˜¯å¦å¯ç”¨"""
        try:
            response = self.session.get(f"{self.api_base_url}/health", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def measure_response_time(self, endpoint: str, method: str = 'GET', data: dict = None) -> Dict[str, Any]:
        """æµ‹é‡å•ä¸ªè¯·æ±‚çš„å“åº”æ—¶é—´"""
        start_time = time.time()
        
        try:
            if method.upper() == 'GET':
                response = self.session.get(f"{self.api_base_url}{endpoint}", timeout=30)
            elif method.upper() == 'POST':
                response = self.session.post(f"{self.api_base_url}{endpoint}", json=data, timeout=30)
            else:
                raise ValueError(f"Unsupported method: {method}")
            
            end_time = time.time()
            response_time = end_time - start_time
            
            return {
                'success': True,
                'status_code': response.status_code,
                'response_time': response_time,
                'response_size': len(response.content),
                'timestamp': start_time
            }
            
        except Exception as e:
            end_time = time.time()
            return {
                'success': False,
                'error': str(e),
                'response_time': end_time - start_time,
                'timestamp': start_time
            }
    
    def measure_throughput(self, endpoint: str, method: str, data: dict, duration_seconds: int, concurrent_users: int) -> Dict[str, Any]:
        """æµ‹é‡ç³»ç»Ÿååé‡"""
        results = []
        start_time = time.time()
        
        def worker():
            """å·¥ä½œçº¿ç¨‹"""
            session = requests.Session()
            while time.time() - start_time < duration_seconds:
                try:
                    if method.upper() == 'GET':
                        response = session.get(f"{self.api_base_url}{endpoint}", timeout=10)
                    else:
                        response = session.post(f"{self.api_base_url}{endpoint}", json=data, timeout=10)
                    
                    results.append({
                        'timestamp': time.time(),
                        'status_code': response.status_code,
                        'response_time': response.elapsed.total_seconds(),
                        'success': response.status_code < 400
                    })
                    
                except Exception as e:
                    results.append({
                        'timestamp': time.time(),
                        'error': str(e),
                        'response_time': 0,
                        'success': False
                    })
                
                time.sleep(0.1)  # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡åº¦è´Ÿè½½
        
        # å¯åŠ¨å¹¶å‘å·¥ä½œçº¿ç¨‹
        threads = []
        for _ in range(concurrent_users):
            thread = threading.Thread(target=worker)
            thread.start()
            threads.append(thread)
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # åˆ†æç»“æœ
        total_requests = len(results)
        successful_requests = len([r for r in results if r.get('success', False)])
        failed_requests = total_requests - successful_requests
        
        if successful_requests > 0:
            avg_response_time = statistics.mean([r['response_time'] for r in results if r.get('success', False)])
            throughput = successful_requests / duration_seconds
        else:
            avg_response_time = 0
            throughput = 0
        
        return {
            'total_requests': total_requests,
            'successful_requests': successful_requests,
            'failed_requests': failed_requests,
            'success_rate': successful_requests / total_requests if total_requests > 0 else 0,
            'avg_response_time': avg_response_time,
            'throughput': throughput,
            'duration': duration_seconds,
            'concurrent_users': concurrent_users,
            'raw_results': results
        }
    
    def monitor_resource_usage(self, duration_seconds: int, interval_seconds: int = 1) -> List[Dict[str, Any]]:
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        resource_data = []
        start_time = time.time()
        
        while time.time() - start_time < duration_seconds:
            try:
                # è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯
                cpu_percent = psutil.cpu_percent(interval=0.1)
                memory = psutil.virtual_memory()
                disk = psutil.disk_usage('/')
                
                # è·å–APIæœåŠ¡å™¨èµ„æºä¿¡æ¯
                try:
                    response = self.session.get(f"{self.api_base_url}/system/resources", timeout=2)
                    if response.status_code == 200:
                        api_resources = response.json()
                    else:
                        api_resources = {}
                except:
                    api_resources = {}
                
                resource_data.append({
                    'timestamp': time.time(),
                    'system_cpu_percent': cpu_percent,
                    'system_memory_percent': memory.percent,
                    'system_disk_percent': (disk.used / disk.total) * 100,
                    'api_cpu_percent': api_resources.get('cpu_percent', 0),
                    'api_memory_percent': api_resources.get('memory_percent', 0),
                    'api_active_tasks': api_resources.get('active_tasks', 0)
                })
                
            except Exception as e:
                resource_data.append({
                    'timestamp': time.time(),
                    'error': str(e)
                })
            
            time.sleep(interval_seconds)
        
        return resource_data

class TestResponseTimePerformance(unittest.TestCase):
    """å“åº”æ—¶é—´æ€§èƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.tester = PerformanceBenchmarkTester()
        
        if not self.tester.check_api_availability():
            self.skipTest("APIæœåŠ¡ä¸å¯ç”¨")
    
    def tearDown(self):
        """æµ‹è¯•åçš„æ¸…ç†"""
        self.tester.cleanup()
    
    def test_api_endpoint_response_times(self):
        """æµ‹è¯•å„APIç«¯ç‚¹çš„å“åº”æ—¶é—´"""
        print("\nâ±ï¸ æµ‹è¯•APIç«¯ç‚¹å“åº”æ—¶é—´...")
        
        # å®šä¹‰è¦æµ‹è¯•çš„ç«¯ç‚¹
        endpoints = [
            {'name': 'å¥åº·æ£€æŸ¥', 'endpoint': '/health', 'method': 'GET'},
            {'name': 'ç³»ç»Ÿèµ„æº', 'endpoint': '/system/resources', 'method': 'GET'},
            {'name': 'é”™è¯¯ç»Ÿè®¡', 'endpoint': '/system/errors/stats', 'method': 'GET'},
            {'name': 'ä»»åŠ¡åˆ—è¡¨', 'endpoint': '/system/tasks', 'method': 'GET'},
            {
                'name': 'è§†é¢‘è½¬å½•æäº¤', 
                'endpoint': '/generate_text_from_video', 
                'method': 'POST',
                'data': {'video_url': self.tester.test_video_url}
            }
        ]
        
        response_times = {}
        
        for endpoint_config in endpoints:
            print(f"   ğŸ“Š æµ‹è¯• {endpoint_config['name']}...")
            
            # è¿›è¡Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
            times = []
            for i in range(5):
                result = self.tester.measure_response_time(
                    endpoint_config['endpoint'],
                    endpoint_config['method'],
                    endpoint_config.get('data')
                )
                
                if result['success']:
                    times.append(result['response_time'])
                    if result.get('status_code') == 200 and endpoint_config['method'] == 'POST':
                        # å¦‚æœæ˜¯POSTè¯·æ±‚ä¸”æˆåŠŸï¼Œè®°å½•ä»»åŠ¡IDä»¥ä¾¿æ¸…ç†
                        try:
                            response_data = self.tester.session.get(
                                f"{self.tester.api_base_url}{endpoint_config['endpoint']}",
                                json=endpoint_config.get('data')
                            ).json()
                            task_id = response_data.get('task_id')
                            if task_id:
                                self.tester.test_tasks.append(task_id)
                        except:
                            pass
                else:
                    print(f"      âš ï¸ ç¬¬{i+1}æ¬¡è¯·æ±‚å¤±è´¥: {result.get('error')}")
                
                time.sleep(0.5)  # çŸ­æš‚ä¼‘æ¯
            
            if times:
                avg_time = statistics.mean(times)
                min_time = min(times)
                max_time = max(times)
                
                response_times[endpoint_config['name']] = {
                    'avg': avg_time,
                    'min': min_time,
                    'max': max_time,
                    'samples': len(times)
                }
                
                print(f"      âœ… å¹³å‡: {avg_time:.3f}s, æœ€å°: {min_time:.3f}s, æœ€å¤§: {max_time:.3f}s")
            else:
                print(f"      âŒ æ‰€æœ‰è¯·æ±‚éƒ½å¤±è´¥")
        
        # éªŒè¯å“åº”æ—¶é—´åœ¨åˆç†èŒƒå›´å†…
        for name, times in response_times.items():
            self.assertLess(times['avg'], 10.0, f"{name} å¹³å‡å“åº”æ—¶é—´åº”å°äº10ç§’")
        
        print("   ğŸ‰ å“åº”æ—¶é—´æµ‹è¯•å®Œæˆ")

class TestThroughputPerformance(unittest.TestCase):
    """ååé‡æ€§èƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.tester = PerformanceBenchmarkTester()
        
        if not self.tester.check_api_availability():
            self.skipTest("APIæœåŠ¡ä¸å¯ç”¨")
    
    def tearDown(self):
        """æµ‹è¯•åçš„æ¸…ç†"""
        self.tester.cleanup()
    
    def test_health_check_throughput(self):
        """æµ‹è¯•å¥åº·æ£€æŸ¥ç«¯ç‚¹çš„ååé‡"""
        print("\nğŸš€ æµ‹è¯•å¥åº·æ£€æŸ¥ååé‡...")
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [1, 5, 10]
        duration = 30  # 30ç§’æµ‹è¯•
        
        for concurrent_users in concurrency_levels:
            print(f"   ğŸ“Š æµ‹è¯•å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
            
            result = self.tester.measure_throughput(
                '/health', 'GET', None, duration, concurrent_users
            )
            
            print(f"      æ€»è¯·æ±‚æ•°: {result['total_requests']}")
            print(f"      æˆåŠŸç‡: {result['success_rate']:.2%}")
            print(f"      ååé‡: {result['throughput']:.2f} è¯·æ±‚/ç§’")
            print(f"      å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']:.3f}ç§’")
            
            # éªŒè¯åŸºæœ¬æ€§èƒ½æŒ‡æ ‡
            self.assertGreater(result['success_rate'], 0.8, "å¥åº·æ£€æŸ¥æˆåŠŸç‡åº”å¤§äº80%")
            self.assertGreater(result['throughput'], 1.0, "å¥åº·æ£€æŸ¥ååé‡åº”å¤§äº1è¯·æ±‚/ç§’")
            self.assertLess(result['avg_response_time'], 2.0, "å¥åº·æ£€æŸ¥å¹³å‡å“åº”æ—¶é—´åº”å°äº2ç§’")
        
        print("   ğŸ‰ å¥åº·æ£€æŸ¥ååé‡æµ‹è¯•å®Œæˆ")
    
    def test_video_processing_throughput(self):
        """æµ‹è¯•è§†é¢‘å¤„ç†ä»»åŠ¡çš„ååé‡"""
        print("\nğŸ¬ æµ‹è¯•è§†é¢‘å¤„ç†ååé‡...")
        
        # æµ‹è¯•è§†é¢‘è½¬å½•ä»»åŠ¡çš„æäº¤ååé‡
        print("   ğŸ“Š æµ‹è¯•è½¬å½•ä»»åŠ¡æäº¤ååé‡...")
        
        result = self.tester.measure_throughput(
            '/generate_text_from_video', 
            'POST', 
            {'video_url': self.tester.test_video_url},
            20,  # 20ç§’æµ‹è¯•
            3    # 3ä¸ªå¹¶å‘ç”¨æˆ·
        )
        
        print(f"      æ€»è¯·æ±‚æ•°: {result['total_requests']}")
        print(f"      æˆåŠŸç‡: {result['success_rate']:.2%}")
        print(f"      ååé‡: {result['throughput']:.2f} è¯·æ±‚/ç§’")
        print(f"      å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']:.3f}ç§’")
        
        # éªŒè¯è§†é¢‘å¤„ç†ä»»åŠ¡çš„åŸºæœ¬æ€§èƒ½
        # ç”±äºè§†é¢‘å¤„ç†ä»»åŠ¡å¯èƒ½å—èµ„æºé™åˆ¶ï¼Œæˆ‘ä»¬æ”¾å®½è¦æ±‚
        if result['successful_requests'] > 0:
            self.assertLess(result['avg_response_time'], 5.0, "è§†é¢‘å¤„ç†ä»»åŠ¡æäº¤å“åº”æ—¶é—´åº”å°äº5ç§’")
        
        print("   ğŸ‰ è§†é¢‘å¤„ç†ååé‡æµ‹è¯•å®Œæˆ")

class TestResourceUsagePerformance(unittest.TestCase):
    """èµ„æºä½¿ç”¨æ€§èƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.tester = PerformanceBenchmarkTester()
        
        if not self.tester.check_api_availability():
            self.skipTest("APIæœåŠ¡ä¸å¯ç”¨")
    
    def tearDown(self):
        """æµ‹è¯•åçš„æ¸…ç†"""
        self.tester.cleanup()
    
    def test_resource_usage_under_load(self):
        """æµ‹è¯•è´Ÿè½½ä¸‹çš„èµ„æºä½¿ç”¨æƒ…å†µ"""
        print("\nğŸ’» æµ‹è¯•è´Ÿè½½ä¸‹çš„èµ„æºä½¿ç”¨...")
        
        # å¯åŠ¨èµ„æºç›‘æ§
        print("   ğŸ“Š å¯åŠ¨èµ„æºç›‘æ§...")
        
        def run_load_test():
            """è¿è¡Œè´Ÿè½½æµ‹è¯•"""
            return self.tester.measure_throughput(
                '/health', 'GET', None, 30, 5
            )
        
        # åŒæ—¶è¿è¡Œè´Ÿè½½æµ‹è¯•å’Œèµ„æºç›‘æ§
        with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
            # å¯åŠ¨èµ„æºç›‘æ§
            resource_future = executor.submit(self.tester.monitor_resource_usage, 35, 2)
            
            # ç­‰å¾…2ç§’åå¯åŠ¨è´Ÿè½½æµ‹è¯•
            time.sleep(2)
            load_future = executor.submit(run_load_test)
            
            # è·å–ç»“æœ
            load_result = load_future.result()
            resource_data = resource_future.result()
        
        # åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ
        print("   ğŸ“ˆ åˆ†æèµ„æºä½¿ç”¨æƒ…å†µ...")
        
        valid_resource_data = [r for r in resource_data if 'error' not in r]
        
        if valid_resource_data:
            cpu_values = [r.get('system_cpu_percent', 0) for r in valid_resource_data]
            memory_values = [r.get('system_memory_percent', 0) for r in valid_resource_data]
            
            if cpu_values and memory_values:
                avg_cpu = statistics.mean(cpu_values)
                max_cpu = max(cpu_values)
                avg_memory = statistics.mean(memory_values)
                max_memory = max(memory_values)
                
                print(f"      å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
                print(f"      æœ€å¤§CPUä½¿ç”¨ç‡: {max_cpu:.1f}%")
                print(f"      å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {avg_memory:.1f}%")
                print(f"      æœ€å¤§å†…å­˜ä½¿ç”¨ç‡: {max_memory:.1f}%")
                
                # éªŒè¯èµ„æºä½¿ç”¨åœ¨åˆç†èŒƒå›´å†…
                self.assertLess(max_cpu, 95.0, "CPUä½¿ç”¨ç‡ä¸åº”è¶…è¿‡95%")
                self.assertLess(max_memory, 90.0, "å†…å­˜ä½¿ç”¨ç‡ä¸åº”è¶…è¿‡90%")
        
        print(f"   ğŸš€ è´Ÿè½½æµ‹è¯•ç»“æœ: {load_result['throughput']:.2f} è¯·æ±‚/ç§’")
        print("   ğŸ‰ èµ„æºä½¿ç”¨æ€§èƒ½æµ‹è¯•å®Œæˆ")

class TestScalabilityPerformance(unittest.TestCase):
    """å¯æ‰©å±•æ€§æ€§èƒ½æµ‹è¯•"""
    
    def setUp(self):
        """æµ‹è¯•å‰çš„è®¾ç½®"""
        self.tester = PerformanceBenchmarkTester()
        
        if not self.tester.check_api_availability():
            self.skipTest("APIæœåŠ¡ä¸å¯ç”¨")
    
    def tearDown(self):
        """æµ‹è¯•åçš„æ¸…ç†"""
        self.tester.cleanup()
    
    def test_concurrent_user_scalability(self):
        """æµ‹è¯•å¹¶å‘ç”¨æˆ·å¯æ‰©å±•æ€§"""
        print("\nğŸ“ˆ æµ‹è¯•å¹¶å‘ç”¨æˆ·å¯æ‰©å±•æ€§...")
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«ä¸‹çš„æ€§èƒ½
        concurrency_levels = [1, 2, 5, 10, 15]
        duration = 20  # æ¯ä¸ªçº§åˆ«æµ‹è¯•20ç§’
        
        scalability_results = {}
        
        for concurrent_users in concurrency_levels:
            print(f"   ğŸ‘¥ æµ‹è¯•å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
            
            result = self.tester.measure_throughput(
                '/health', 'GET', None, duration, concurrent_users
            )
            
            scalability_results[concurrent_users] = {
                'throughput': result['throughput'],
                'success_rate': result['success_rate'],
                'avg_response_time': result['avg_response_time']
            }
            
            print(f"      ååé‡: {result['throughput']:.2f} è¯·æ±‚/ç§’")
            print(f"      æˆåŠŸç‡: {result['success_rate']:.2%}")
            print(f"      å¹³å‡å“åº”æ—¶é—´: {result['avg_response_time']:.3f}ç§’")
            
            # çŸ­æš‚ä¼‘æ¯è®©ç³»ç»Ÿæ¢å¤
            time.sleep(5)
        
        # åˆ†æå¯æ‰©å±•æ€§
        print("   ğŸ“Š å¯æ‰©å±•æ€§åˆ†æ:")
        
        throughputs = [scalability_results[level]['throughput'] for level in concurrency_levels]
        response_times = [scalability_results[level]['avg_response_time'] for level in concurrency_levels]
        
        # è®¡ç®—ååé‡å¢é•¿ç‡
        if len(throughputs) >= 2:
            throughput_growth = (throughputs[-1] - throughputs[0]) / throughputs[0] if throughputs[0] > 0 else 0
            print(f"      ååé‡å¢é•¿ç‡: {throughput_growth:.2%}")
            
            # ç†æƒ³æƒ…å†µä¸‹ï¼Œååé‡åº”è¯¥éšå¹¶å‘æ•°å¢åŠ è€Œå¢åŠ ï¼ˆè‡³å°‘åœ¨ä½å¹¶å‘æ—¶ï¼‰
            if throughput_growth > 0:
                print("      âœ… ç³»ç»Ÿæ˜¾ç¤ºå‡ºè‰¯å¥½çš„å¯æ‰©å±•æ€§")
            else:
                print("      âš ï¸ ç³»ç»Ÿå¯æ‰©å±•æ€§æœ‰é™")
        
        # æ£€æŸ¥å“åº”æ—¶é—´æ˜¯å¦éšå¹¶å‘æ•°åˆç†å¢é•¿
        if len(response_times) >= 2:
            response_time_growth = (response_times[-1] - response_times[0]) / response_times[0] if response_times[0] > 0 else 0
            print(f"      å“åº”æ—¶é—´å¢é•¿ç‡: {response_time_growth:.2%}")
            
            # å“åº”æ—¶é—´å¢é•¿åº”è¯¥æ˜¯åˆç†çš„
            if response_time_growth < 5.0:  # 500%ä»¥å†…çš„å¢é•¿æ˜¯å¯æ¥å—çš„
                print("      âœ… å“åº”æ—¶é—´å¢é•¿åœ¨åˆç†èŒƒå›´å†…")
            else:
                print("      âš ï¸ å“åº”æ—¶é—´å¢é•¿è¿‡å¿«ï¼Œå¯èƒ½å­˜åœ¨æ€§èƒ½ç“¶é¢ˆ")
        
        print("   ğŸ‰ å¹¶å‘ç”¨æˆ·å¯æ‰©å±•æ€§æµ‹è¯•å®Œæˆ")

def run_performance_benchmarks():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 80)
    
    # åˆ›å»ºæµ‹è¯•å¥—ä»¶
    test_suite = unittest.TestSuite()
    
    # æ·»åŠ æµ‹è¯•ç±»
    test_classes = [
        TestResponseTimePerformance,
        TestThroughputPerformance,
        TestResourceUsagePerformance,
        TestScalabilityPerformance
    ]
    
    for test_class in test_classes:
        tests = unittest.TestLoader().loadTestsFromTestCase(test_class)
        test_suite.addTests(tests)
    
    # è¿è¡Œæµ‹è¯•
    runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)
    result = runner.run(test_suite)
    
    # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
    print("\n" + "=" * 80)
    print("ğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"   æ€»æµ‹è¯•æ•°: {result.testsRun}")
    print(f"   æˆåŠŸ: {result.testsRun - len(result.failures) - len(result.errors)}")
    print(f"   å¤±è´¥: {len(result.failures)}")
    print(f"   é”™è¯¯: {len(result.errors)}")
    print(f"   è·³è¿‡: {len(result.skipped) if hasattr(result, 'skipped') else 0}")
    
    if result.wasSuccessful():
        print("ğŸ‰ æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡ï¼")
        print("âœ… ç³»ç»Ÿæ€§èƒ½è¡¨ç°è‰¯å¥½")
    else:
        print("âš ï¸ éƒ¨åˆ†æ€§èƒ½åŸºå‡†æµ‹è¯•å¤±è´¥")
        
        if result.failures:
            print("\nâŒ å¤±è´¥çš„æµ‹è¯•:")
            for test, traceback in result.failures:
                print(f"   - {test}")
        
        if result.errors:
            print("\nğŸ’¥ é”™è¯¯çš„æµ‹è¯•:")
            for test, traceback in result.errors:
                print(f"   - {test}")
    
    # æ€§èƒ½å»ºè®®
    print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    print("   - å®šæœŸè¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•ç›‘æ§ç³»ç»Ÿæ€§èƒ½")
    print("   - æ ¹æ®è´Ÿè½½æƒ…å†µè°ƒæ•´ç³»ç»Ÿèµ„æºé…ç½®")
    print("   - ç›‘æ§å…³é”®æ€§èƒ½æŒ‡æ ‡å¹¶è®¾ç½®å‘Šè­¦")
    print("   - è€ƒè™‘å®æ–½ç¼“å­˜ç­–ç•¥æé«˜å“åº”é€Ÿåº¦")
    print("   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œæ–‡ä»¶I/Oæ“ä½œ")
    
    return result.wasSuccessful()

if __name__ == "__main__":
    import sys
    success = run_performance_benchmarks()
    sys.exit(0 if success else 1)